Below is a more detailed, MRO-style description of what was tested and what the results mean, using only the fields shown in your outputs and the results for 2024Q2, 2024Q4, and 2025Q2.
MRO independently executed a targeted data quality assessment to confirm that the core inputs used in monitoring and downstream analysis are complete, valid, and internally consistent. The assessment focused only on SCORE, LOANPURPOSECATEGORY, TARGET, SCORE_DATE, APPID, and MODEL_SCORE_ID because these fields directly support score distribution monitoring, segmentation by purpose, outcome-based performance testing, and temporal alignment of score observations. Fields not used in downstream model monitoring logic were intentionally excluded to keep the review aligned with how the data is actually used and to avoid placing control reliance on irrelevant columns.
For each monitoring period, MRO applied the same set of checks. First, MRO confirmed that each required field is present and reviewed record counts to ensure the dataset is populated. Second, MRO assessed missingness by counting nulls and reporting the missing percentage for each field. Third, MRO validated data types to ensure fields are stored in the expected formats for analysis, including numeric types for SCORE and TARGET, object/string types for identifier and categorical fields, and datetime for SCORE_DATE. Fourth, MRO performed range and reasonableness testing on numeric fields, including boundary checks and distribution summaries. Fifth, MRO performed domain checks on LOANPURPOSECATEGORY to confirm values are constrained to the expected set and to identify any unexpected labels. Sixth, MRO tested date validity for SCORE_DATE by confirming all values are parseable, fall within the expected monitoring window, and are not future-dated relative to the run date. Finally, MRO assessed uniqueness and duplication. MRO tested duplication at the application level (APPID) to understand repeat appearances, and then tested duplication at the model scoring event level (MODEL_SCORE_ID) to confirm whether each record represents a unique scoring event.
For 2024Q2, MRO tested 123,118 records using SCORE, LOANPURPOSECATEGORY, TARGET, SCORE_DATE, and APPID, and separately evaluated record-level uniqueness using MODEL_SCORE_ID. Missingness testing indicated 0 missing values for all assessed fields. Data type checks confirmed SCORE is numeric (float), TARGET is numeric (integer), SCORE_DATE is a timezone-aware datetime, and APPID and LOANPURPOSECATEGORY are stored as object fields, which is appropriate for identifiers and categorical labels. SCORE range checks confirmed SCORE is fully within the expected probability bounds, with no values below 0 and no values above 1. The observed score distribution summary for 2024Q2 was min 0.0005484, median 0.06509281, and max 0.99234915, with a 1st percentile of 0.0018986606 and a 99th percentile of 0.9510964355. TARGET validity checks confirmed TARGET contains only binary values, with 123,109 records coded as 0 and 9 records coded as 1, and no values outside {0,1}. LOANPURPOSECATEGORY checks showed four observed categories and no missing values; the most frequent values were Auto (39,631), Debt Consolidation (33,228), Home Improvement (28,352), and Other (21,907). The “outside expected set” flag equaled 21,907 in 2024Q2, which corresponds to the “Other” label being flagged when the expected set is defined differently (for example, “others” instead of “Other”). This result indicates a label standardization issue rather than missing or malformed data and is resolved by aligning the expected category spelling/casing to the dataset’s actual values. SCORE_DATE validity checks confirmed all dates were parseable and fell within the quarter window, with SCORE_DATE min 2024-04-01 00:01:03+00:00 and max 2024-06-30 23:59:19+00:00. No future-dated SCORE_DATE records were identified. Duplicate testing showed APPID is not unique at the row level in 2024Q2, with 29,998 duplicate rows (24.3652%). This indicates repeated APPIDs are present and should not be treated as a record-level key. MRO therefore evaluated MODEL_SCORE_ID, and the duplication test showed 0 duplicate MODEL_SCORE_ID rows and 123,118 unique MODEL_SCORE_ID values out of 123,118 total rows, indicating each row corresponds to a unique scoring event and the dataset is structurally consistent for event-level monitoring.
For 2024Q4, MRO tested 279,773 records using the same set of fields and checks. Missingness testing again indicated 0 missing values across SCORE, LOANPURPOSECATEGORY, TARGET, SCORE_DATE, and APPID. Data types were consistent with expectations, including SCORE_DATE stored as timezone-aware datetime. SCORE range checks again indicated no out-of-bound values, with SCORE < 0 equal to 0 and SCORE > 1 equal to 0. The 2024Q4 score distribution summary was min 0.00056776, median 0.09752826, and max 0.99420834, with a 1st percentile of 0.0020640392 and a 99th percentile of 0.9639992508. TARGET checks confirmed valid binary coding with 279,690 records coded as 0 and 83 records coded as 1, with no invalid values outside {0,1}. LOANPURPOSECATEGORY checks again showed four observed categories and no missing values; top categories were Auto (91,587), Debt Consolidation (82,915), Other (56,493), and Home Improvement (48,778). The “outside expected set” count equaled 56,493, again consistent with the “Other” label being flagged when the expected set differs in spelling/casing. SCORE_DATE validity checks confirmed full parseability and correct window alignment for the period, with SCORE_DATE min 2024-07-01 00:00:27+00:00 and max 2024-12-31 23:58:47+00:00, and 0 future-dated records. Duplicate testing showed APPID duplication persists, with 54,792 duplicate rows (19.5844%). This confirms APPID is not an appropriate unique key for record-level integrity checks. MRO’s MODEL_SCORE_ID duplication test again showed 0 duplicate rows and 279,773 unique MODEL_SCORE_ID values out of 279,773 total rows, confirming record-level uniqueness at the scoring-event level.
For 2025Q2, MRO tested 322,020 records and repeated the same checks. Missingness testing indicated 0 missing values for all assessed fields. Data types remained consistent, including timezone-aware datetime for SCORE_DATE. SCORE range checks again showed no invalid values, with SCORE < 0 equal to 0 and SCORE > 1 equal to 0. The 2025Q2 score distribution summary was min 0.00052358, median 0.08332143, and max 0.99135286, with a 1st percentile of 0.0020391837 and a 99th percentile of 0.95837438. TARGET checks again confirmed valid binary coding with 321,818 records coded as 0 and 202 records coded as 1, and no values outside {0,1}. LOANPURPOSECATEGORY checks showed four observed categories and no missingness; top values were Auto (100,368), Debt Consolidation (96,505), Home Improvement (62,951), and Other (62,196). The “outside expected set” count equaled 62,196, consistent with the same label standardization issue described above. SCORE_DATE validity checks confirmed all values are parseable and aligned to the period window, with SCORE_DATE min 2025-01-01 00:00:52+00:00 and max 2025-06-30 23:59:23+00:00, and 0 future-dated records. Duplicate testing again showed APPID duplication, with 66,902 duplicate rows (20.7757%), reinforcing that APPID should not be interpreted as a record-level unique identifier. The MODEL_SCORE_ID duplication test again showed 0 duplicate rows and 322,020 unique MODEL_SCORE_ID values out of 322,020 total rows, confirming each record represents a unique score event.
Across the three monitoring periods, the results collectively indicate that the assessed datasets are complete and analytically usable for monitoring. All required fields showed 0 missingness, numeric fields met expected constraints, categorical fields were populated and consistent, and date fields were parseable and within expected quarter windows with no future-dated values. APPID duplication is present and is not treated as a data defect because it reflects repeated application identifiers in the dataset; instead, record-level uniqueness was confirmed using MODEL_SCORE_ID, which showed no duplicates in any period. The only item consistently flagged by domain checking is the LOANPURPOSECATEGORY “Other” label being outside an expected set that appears to use a different spelling/casing (for example, “others”). This is a standardization issue in the expected-value list rather than a missingness or integrity failure in the data and can be addressed by aligning the expected category mapping to the dataset’s observed labels.
If you want, I can rewrite this into your exact report section format (one paragraph per check type, then one paragraph summarizing conclusion), but the content above is already in MRO tone and ties directly to the numbers you produced.



===========



Data Treatment and Exclusion Assessment for Ongoing Monitoring (OGM) Data
The model under review is a vendor-developed model. Vendor-level data preprocessing, cleaning, and record-level exclusion logic applied prior to delivery of OGM datasets were not provided to MRO. The monitoring code and materials made available to MRO do not include upstream data preparation steps, feature engineering logic, or vendor-applied data conditioning for OGM data. As a result, MRO does not assess or opine on vendor preprocessing and limits its review to the OGM datasets as received and the monitoring code used to compute stability and performance metrics.
Within the scope of materials provided, MRO did not observe any preprocessing logic that modifies or cleans the OGM dataset itself (for example, imputation, winsorization, outlier removal, score adjustment, or target recoding). The monitoring code uses OGM data directly as inputs to analytical routines. However, the code applies several explicit data exclusions and conditioning steps at the point of analysis, which determine the effective population and feature set used for OGM monitoring outputs. These exclusions are methodological in nature and should be documented because they materially affect the monitoring results.
First, the monitoring code applies a time-window restriction using SCORE_DATE. OGM observations are filtered to those falling within a specified monitoring window defined by begin_time and end_time. All records outside the window are excluded from monitoring calculations. This exclusion establishes the quarterly or semiannual OGM population and is a standard step for period-specific monitoring.
Second, the code applies population-definition exclusions through a configurable subset parameter. When monitoring is run on a funded subset, observations with zero or missing funded amount are excluded. When run on an approved subset, observations without an approval date are excluded. When run on the all subset, these exclusions are not applied. These steps are not data cleaning; they define the population eligible for monitoring and result in different effective datasets depending on the selected subset.
Third, the monitoring code applies segment-level exclusions using LOANPURPOSECATEGORY. Each monitoring run filters the OGM dataset to a single loan purpose segment (for example, Auto, Debt Consolidation, Home Improvement, or Others). All records outside the selected segment are excluded from the analysis. As a result, stability metrics are segment-conditional rather than portfolio-wide.
Fourth, the code applies a score-based truncation for PSI calculations, which is one of the most material exclusions observed. For population stability analysis, model scores are ranked, and a reference population is defined using a fixed score percentile (for example, the lowest-risk decile). The OGM population used for PSI is then restricted to observations with scores less than or equal to the maximum score observed in the reference population. All OGM observations with scores above this cutoff are excluded from PSI computation. This truncation is explicitly designed to ensure comparability between the reference and monitoring populations and is documented as part of the PSI methodology, but it means that PSI results do not reflect the full OGM score distribution.
Fifth, the reference population itself is subject to additional conditioning. For certain stability and explainability calculations, the reference set is restricted to “approved good borrowers,” typically defined as observations with non-default outcomes and, in funded contexts, non-zero funded amounts. This conditioning does not alter the underlying OGM dataset but excludes records from the reference group used to benchmark stability metrics.
Sixth, for outcome-based analyses, the code applies outcome availability exclusions. Observations with missing performance outcomes are dropped prior to target construction. Where feature data and outcome data are combined, the code further restricts the population through index intersection logic, retaining only observations present in both datasets. Records that do not align across tables are implicitly excluded without separate reporting of pre- and post-filter counts.
Seventh, for attribute-level PSI calculations, the code applies feature-level exclusions. Features are expanded from the DATA payload and then restricted to a predefined feature universe, such as model input features and those present in the feature importance file. In addition, attributes with high missingness are excluded from PSI calculations, consistent with documented methodology and the known sensitivity of PSI to sparse variables. These exclusions operate at the variable level rather than the record level but materially define which attributes contribute to stability results.
In parallel with the review of developer monitoring code, MRO performed independent data quality testing on the OGM datasets provided, limited to observable properties of the data. Testing assessed missingness, data type consistency, value ranges, identifier behavior, and score date validity for key monitoring fields. Results indicate no material missing values, no invalid score ranges, consistent target coding, valid date formats within expected monitoring windows, and no anomalies indicating data corruption.
Conclusion
Vendor-level preprocessing and exclusion logic applied to OGM data were not provided to MRO and are therefore outside the scope of this review. Within the monitoring code available to MRO, no preprocessing or cleaning of OGM data as received was observed. However, the monitoring implementation applies multiple explicit, analysis-time data exclusions, including time-window filtering, population-definition restrictions, segment-based subsetting, score-based truncation for PSI, outcome availability filtering, dataset alignment via index intersection, and feature-level exclusions for attribute PSI. These steps are methodological and define the effective population and feature set used in OGM monitoring.
Based on the observed monitoring logic and MRO’s independent data quality testing, the OGM data used for monitoring appear internally consistent and appropriately handled for their intended monitoring purpose, subject to the documented analytical exclusions described above.


=============

Data processing summary 


